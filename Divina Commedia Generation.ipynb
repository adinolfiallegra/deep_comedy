{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divina Commedia Demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to showcase our project results here a demo to run our main script \"generete_dante.py\" and see our Canto printed out with all the requirements needed as:\n",
    "- terzine structure \n",
    "- hendecasyllable verse\n",
    "- rhyme scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PROCESSING\n",
    "We first want to show how we performed some data processing in order to get our latest canto, for this reason in the following session we will share different types of Divina Commedia text in order to all the phases we processed before to get out CANTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/allegraadinolfi/Desktop/UNIBO/DEEP LEARNING/deep_comedy/dante_by_tonedrev_syl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dante_by_tonedrev_syl.text_processing import clean_comedy, special_tokens, prettify_text\n",
    "from dante_by_tonedrev_syl.tone import ToneTagger\n",
    "from dante_by_tonedrev_syl.syllabification import syllabify_verse, syllabify_verse_prettify\n",
    "from dante_by_rev_syl.data_preparation import text_in_rev_syls\n",
    "from dante_by_rev_syl.data_preparation import text_in_syls_rhyme\n",
    "\n",
    "\n",
    "working_dir = os.path.join(os.path.abspath('.'), 'dante_by_tonedrev_syl')\n",
    "divine_comedy_file = os.path.join(os.path.dirname(working_dir), \"divina_commedia\", \"divina_commedia_accent_UTF-8.txt\")\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_canto>\n",
      "<start_of_terzina>\n",
      "nel <word_sep> mezzo <word_sep> del <word_sep> cammin <word_sep> di <word_sep> nostra <word_sep> vita <end_of_verso>\n",
      "mi <word_sep> ritrovai <word_sep> per <word_sep> una <word_sep> selva <word_sep> oscura <end_of_verso>\n",
      "ché <word_sep> la <word_sep> diritta <word_sep> via <word_sep> era <word_sep> smarrita <end_of_verso>\n",
      "<end_of_terzina>\n",
      "<start_of_terzina>\n",
      "ahi <word_sep> quanto <word_sep> a <word_sep> dir <word_sep> qual <word_sep> era <word_sep> è <word_sep> cosa <word_sep> dura <end_of_verso>\n",
      "esta <word_sep> selva <word_sep> selvaggia <word_sep> e <word_sep\n",
      "Special tokens used:  dict_values(['<start_of_canto>', '<end_of_canto>', '<start_of_terzina>', '<end_of_terzina>', '<end_of_verso>', '<word_sep>'])\n"
     ]
    }
   ],
   "source": [
    "from dante_by_tonedrev_syl.text_processing import clean_comedy, special_tokens, prettify_text\n",
    "#reading the original divine comedy \n",
    "with open(divine_comedy_file,\"r\") as f:\n",
    "    divine_comedy = f.read()\n",
    "#cleaning the divine comedy with our functions    \n",
    "divine_comedy = clean_comedy(divine_comedy, special_tokens)\n",
    "print(divine_comedy[:600])\n",
    "print(\"Special tokens used: \", special_tokens.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ToneNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 32)            1088      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1024)              2232320   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 30)                30750     \n",
      "=================================================================\n",
      "Total params: 2,264,158\n",
      "Trainable params: 2,264,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "cànto nel mèzzo del cammìn di nòstra vìta mi ritrovài per ùna sèlva oscùra ché la dirìtta vìa èra smàrrita àhi quànto a dir quàl èra è còsa dùra èsta sèlva selvàggia e àspra e fòrte che nel pènsier rinòva la paùra tant' è amàra che pòco è più mòrte ma per trattàr del ben ch'i' vi trovài dirò de l'àltre còse ch'i' v'ho scòrte io non so ben ridìr com' i' v'intrài tant' èra pièn di sònno a quèl pùnto che la veràce vìa abbandonài ma pòi ch'i' fùi al piè d'un còlle giùnto là dòve terminàva quèlla vàlle che m'àvea di paùra il cor compùnto guardài in àlto e vìdi le sùe spàlle vestìte già de' ràggi del pianèta che mèna drìtto altrùi per ògne càlle allòr fu la paùra un pòco quèta che nel làgo del cor m'èra duràta la nòtte ch'i' passài con tànta pièta e còme quèi che con lèna affannàta uscìto fuòr del pèlago a la rìva si vòlge a l'àcqua perigliòsa e guàta così l'ànimo mìo ch'àncor fuggìva si vòlse a rètro a rimiràr lo pàsso che non lasciò già mài persòna vìva pòi ch'èi posàto un pòco il còrpo làsso riprèsi vìa  "
     ]
    }
   ],
   "source": [
    "from dante_by_tonedrev_syl.tone import ToneTagger\n",
    "divine_comedy_prettified = prettify_text(divine_comedy, special_tokens)\n",
    "#divine_comedy = remove_all_punctuation(divine_comedy)\n",
    "\n",
    "tone_tagger = ToneTagger()\n",
    "divine_comedy_words = divine_comedy_prettified.split()[:200] + ['']\n",
    "\n",
    "for w in divine_comedy_words:\n",
    "    print(tone_tagger.tone(w), flush=True, end=' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syllabified Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nel', '<word_sep>', 'mèz', 'zo', '<word_sep>', 'del', '<word_sep>', 'cam', 'mìn', '<word_sep>', 'di', '<word_sep>', 'nò', 'stra', '<word_sep>', 'vì', 'ta', '<end_of_verso>']\n",
      "['nel', '<word_sep>', 'mez', 'zo', '<word_sep>', 'del', '<word_sep>', 'cam', 'min', '<word_sep>', 'di', '<word_sep>', 'no', 'stra', '<word_sep>', 'vi', 'ta', '<end_of_verso>']\n",
      "['mi', '<word_sep>', 'ri', 'tro', 'vài', '<word_sep>', 'per', '<word_sep>', 'ù', 'na', '<word_sep>', 'sèl', 'va', '<word_sep>', 'o', 'scù', 'ra', '<end_of_verso>']\n",
      "['mi', '<word_sep>', 'ri', 'tro', 'vai', '<word_sep>', 'per', '<word_sep>', 'u', 'na', '<word_sep>', 'sel', 'va<word_sep>o', 'scu', 'ra', '<end_of_verso>']\n",
      "['ché', '<word_sep>', 'la', '<word_sep>', 'di', 'rìt', 'ta', '<word_sep>', 'vì', 'a', '<word_sep>', 'è', 'ra', '<word_sep>', 'smàr', 'ri', 'ta', '<end_of_verso>']\n",
      "['ché', '<word_sep>', 'la', '<word_sep>', 'di', 'rit', 'ta', '<word_sep>', 'vi', 'a<word_sep>e', 'ra', '<word_sep>', 'smar', 'ri', 'ta', '<end_of_verso>']\n",
      "['àhi', '<word_sep>', 'quàn', 'to', '<word_sep>', 'a', '<word_sep>', 'dir', '<word_sep>', 'quàl', '<word_sep>', 'è', 'ra', '<word_sep>', 'è', '<word_sep>', 'cò', 'sa', '<word_sep>', 'dù', 'ra', '<end_of_verso>']\n",
      "['ahi', '<word_sep>', 'quan', 'to<word_sep>a', '<word_sep>', 'dir', '<word_sep>', 'qual', '<word_sep>', 'e', 'ra<word_sep>è', '<word_sep>', 'co', 'sa', '<word_sep>', 'du', 'ra', '<end_of_verso>']\n",
      "['è', 'sta', '<word_sep>', 'sèl', 'va', '<word_sep>', 'sel', 'vàg', 'gia', '<word_sep>', 'e', '<word_sep>', 'à', 'spra', '<word_sep>', 'e', '<word_sep>', 'fòr', 'te', '<end_of_verso>']\n",
      "['e', 'sta', '<word_sep>', 'sel', 'va', '<word_sep>', 'sel', 'vag', 'gia<word_sep>e<word_sep>a', 'spra<word_sep>e', '<word_sep>', 'for', 'te', '<end_of_verso>']\n",
      "['che', '<word_sep>', 'nel', '<word_sep>', 'pèn', 'sier', '<word_sep>', 'ri', 'nò', 'va', '<word_sep>', 'la', '<word_sep>', 'pa', 'ù', 'ra', '<end_of_verso>']\n",
      "['che', '<word_sep>', 'nel', '<word_sep>', 'pen', 'sier', '<word_sep>', 'ri', 'no', 'va', '<word_sep>', 'la', '<word_sep>', 'pa', 'u', 'ra', '<end_of_verso>']\n",
      "['tan', \"t'\", '<word_sep>', 'è', '<word_sep>', 'a', 'mà', 'ra', '<word_sep>', 'che', '<word_sep>', 'pò', 'co', '<word_sep>', 'è', '<word_sep>', 'più', '<word_sep>', 'mòr', 'te', '<end_of_verso>']\n",
      "['tan', \"t'<word_sep>è<word_sep>a\", 'ma', 'ra', '<word_sep>', 'che', '<word_sep>', 'po', 'co<word_sep>è', '<word_sep>', 'più', '<word_sep>', 'mor', 'te', '<end_of_verso>']\n",
      "['ma', '<word_sep>', 'per', '<word_sep>', 'trat', 'tàr', '<word_sep>', 'del', '<word_sep>', 'ben', '<word_sep>', \"ch'i'\", '<word_sep>', 'vi', '<word_sep>', 'tro', 'vài', '<end_of_verso>']\n",
      "['ma', '<word_sep>', 'per', '<word_sep>', 'trat', 'tar', '<word_sep>', 'del', '<word_sep>', 'ben', '<word_sep>', \"ch'i'\", '<word_sep>', 'vi', '<word_sep>', 'tro', 'vai', '<end_of_verso>']\n",
      "['di', 'rò', '<word_sep>', 'de', '<word_sep>', \"l'àl\", 'tre', '<word_sep>', 'cò', 'se', '<word_sep>', \"ch'i'\", '<word_sep>', \"v'ho\", '<word_sep>', 'scòr', 'te', '<end_of_verso>']\n",
      "['di', 'rò', '<word_sep>', 'de', '<word_sep>', \"l'al\", 'tre', '<word_sep>', 'co', 'se', '<word_sep>', \"ch'i'\", '<word_sep>', \"v'ho\", '<word_sep>', 'scor', 'te', '<end_of_verso>']\n",
      "['io', '<word_sep>', 'non', '<word_sep>', 'so', '<word_sep>', 'ben', '<word_sep>', 'ri', 'dìr', '<word_sep>', \"com'\", '<word_sep>', \"i'\", '<word_sep>', \"v'in\", 'trài', '<end_of_verso>']\n",
      "['io', '<word_sep>', 'non', '<word_sep>', 'so', '<word_sep>', 'ben', '<word_sep>', 'ri', 'dir', '<word_sep>', \"com'<word_sep>i'\", '<word_sep>', \"v'in\", 'trai', '<end_of_verso>']\n",
      "['tan', \"t'\", '<word_sep>', 'è', 'ra', '<word_sep>', 'pièn', '<word_sep>', 'di', '<word_sep>', 'sòn', 'no', '<word_sep>', 'a', '<word_sep>', 'quèl', '<word_sep>', 'pùn', 'to', '<end_of_verso>']\n",
      "['tan', \"t'<word_sep>e\", 'ra', '<word_sep>', 'pien', '<word_sep>', 'di', '<word_sep>', 'son', 'no<word_sep>a', '<word_sep>', 'quel', '<word_sep>', 'pun', 'to', '<end_of_verso>']\n",
      "['che', '<word_sep>', 'la', '<word_sep>', 've', 'rà', 'ce', '<word_sep>', 'vì', 'a', '<word_sep>', 'ab', 'ban', 'do', 'nài', '<end_of_verso>']\n",
      "['che', '<word_sep>', 'la', '<word_sep>', 've', 'ra', 'ce', '<word_sep>', 'vi', 'a<word_sep>ab', 'ban', 'do', 'nai', '<end_of_verso>']\n",
      "['ma', '<word_sep>', 'pòi', '<word_sep>', \"ch'i'\", '<word_sep>', 'fùi', '<word_sep>', 'al', '<word_sep>', 'piè', '<word_sep>', \"d'un\", '<word_sep>', 'còl', 'le', '<word_sep>', 'giùn', 'to', '<end_of_verso>']\n",
      "['ma', '<word_sep>', 'poi', '<word_sep>', \"ch'i'\", '<word_sep>', 'fui<word_sep>al', '<word_sep>', 'piè', '<word_sep>', \"d'un\", '<word_sep>', 'col', 'le', '<word_sep>', 'giun', 'to', '<end_of_verso>']\n",
      "['là', '<word_sep>', 'dò', 've', '<word_sep>', 'ter', 'mi', 'nà', 'va', '<word_sep>', 'quèl', 'la', '<word_sep>', 'vàl', 'le', '<end_of_verso>']\n",
      "['là', '<word_sep>', 'do', 've', '<word_sep>', 'ter', 'mi', 'na', 'va', '<word_sep>', 'quel', 'la', '<word_sep>', 'val', 'le', '<end_of_verso>']\n",
      "['che', '<word_sep>', \"m'à\", 've', 'a', '<word_sep>', 'di', '<word_sep>', 'pa', 'ù', 'ra', '<word_sep>', 'il', '<word_sep>', 'cor', '<word_sep>', 'com', 'pùn', 'to', '<end_of_verso>']\n",
      "['che', '<word_sep>', \"m'a\", 've', 'a', '<word_sep>', 'di', '<word_sep>', 'pa', 'u', 'ra<word_sep>il', '<word_sep>', 'cor', '<word_sep>', 'com', 'pun', 'to', '<end_of_verso>']\n"
     ]
    }
   ],
   "source": [
    "from dante_by_tonedrev_syl.syllabification import syllabify_verse, syllabify_verse_prettify\n",
    "divine_comedy_list = divine_comedy.split(\"\\n\")\n",
    "divine_comedy_list = [ line for line in divine_comedy_list if line.strip() not in special_tokens.values() ]\n",
    "for line in divine_comedy_list[:15]:\n",
    "    syllables = syllabify_verse(line, special_tokens, tone_tagger)\n",
    "    size = len(syllables)\n",
    "    print(syllables, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nel', '<word_sep>', 'mèz', 'zo', '<word_sep>', 'del', '<word_sep>', 'cam', 'mìn', '<word_sep>', 'di', '<word_sep>', 'nò', 'stra', '<word_sep>', 'vì', 'ta', '<end_of_verso>']\n",
      "['nel', 'mez', 'zo', 'del', 'cam', 'min', 'di', 'no', 'stra', 'vi', 'ta']\n",
      "['mi', '<word_sep>', 'ri', 'tro', 'vài', '<word_sep>', 'per', '<word_sep>', 'ù', 'na', '<word_sep>', 'sèl', 'va', '<word_sep>', 'o', 'scù', 'ra', '<end_of_verso>']\n",
      "['mi', 'ri', 'tro', 'vai', 'per', 'u', 'na', 'sel', 'va o', 'scu', 'ra']\n",
      "['ché', '<word_sep>', 'la', '<word_sep>', 'di', 'rìt', 'ta', '<word_sep>', 'vì', 'a', '<word_sep>', 'è', 'ra', '<word_sep>', 'smàr', 'ri', 'ta', '<end_of_verso>']\n",
      "['ché', 'la', 'di', 'rit', 'ta', 'vi', 'a e', 'ra', 'smar', 'ri', 'ta']\n",
      "['àhi', '<word_sep>', 'quàn', 'to', '<word_sep>', 'a', '<word_sep>', 'dir', '<word_sep>', 'quàl', '<word_sep>', 'è', 'ra', '<word_sep>', 'è', '<word_sep>', 'cò', 'sa', '<word_sep>', 'dù', 'ra', '<end_of_verso>']\n",
      "['ahi', 'quan', 'to a', 'dir', 'qual', 'e', 'ra è', 'co', 'sa', 'du', 'ra']\n",
      "['è', 'sta', '<word_sep>', 'sèl', 'va', '<word_sep>', 'sel', 'vàg', 'gia', '<word_sep>', 'e', '<word_sep>', 'à', 'spra', '<word_sep>', 'e', '<word_sep>', 'fòr', 'te', '<end_of_verso>']\n",
      "['e', 'sta', 'sel', 'va', 'sel', 'vag', 'gia e a', 'spra e', 'for', 'te']\n",
      "['che', '<word_sep>', 'nel', '<word_sep>', 'pèn', 'sier', '<word_sep>', 'ri', 'nò', 'va', '<word_sep>', 'la', '<word_sep>', 'pa', 'ù', 'ra', '<end_of_verso>']\n",
      "['che', 'nel', 'pen', 'sier', 'ri', 'no', 'va', 'la', 'pa', 'u', 'ra']\n",
      "['tan', \"t'\", '<word_sep>', 'è', '<word_sep>', 'a', 'mà', 'ra', '<word_sep>', 'che', '<word_sep>', 'pò', 'co', '<word_sep>', 'è', '<word_sep>', 'più', '<word_sep>', 'mòr', 'te', '<end_of_verso>']\n",
      "['tan', \"t' è a\", 'ma', 'ra', 'che', 'po', 'co è', 'più', 'mor', 'te']\n",
      "['ma', '<word_sep>', 'per', '<word_sep>', 'trat', 'tàr', '<word_sep>', 'del', '<word_sep>', 'ben', '<word_sep>', \"ch'i'\", '<word_sep>', 'vi', '<word_sep>', 'tro', 'vài', '<end_of_verso>']\n",
      "['ma', 'per', 'trat', 'tar', 'del', 'ben', \"ch'i'\", 'vi', 'tro', 'vai']\n",
      "['di', 'rò', '<word_sep>', 'de', '<word_sep>', \"l'àl\", 'tre', '<word_sep>', 'cò', 'se', '<word_sep>', \"ch'i'\", '<word_sep>', \"v'ho\", '<word_sep>', 'scòr', 'te', '<end_of_verso>']\n",
      "['di', 'rò', 'de', \"l'al\", 'tre', 'co', 'se', \"ch'i'\", \"v'ho\", 'scor', 'te']\n",
      "['io', '<word_sep>', 'non', '<word_sep>', 'so', '<word_sep>', 'ben', '<word_sep>', 'ri', 'dìr', '<word_sep>', \"com'\", '<word_sep>', \"i'\", '<word_sep>', \"v'in\", 'trài', '<end_of_verso>']\n",
      "['io', 'non', 'so', 'ben', 'ri', 'dir', \"com' i'\", \"v'in\", 'trai']\n",
      "['tan', \"t'\", '<word_sep>', 'è', 'ra', '<word_sep>', 'pièn', '<word_sep>', 'di', '<word_sep>', 'sòn', 'no', '<word_sep>', 'a', '<word_sep>', 'quèl', '<word_sep>', 'pùn', 'to', '<end_of_verso>']\n",
      "['tan', \"t' e\", 'ra', 'pien', 'di', 'son', 'no a', 'quel', 'pun', 'to']\n",
      "['che', '<word_sep>', 'la', '<word_sep>', 've', 'rà', 'ce', '<word_sep>', 'vì', 'a', '<word_sep>', 'ab', 'ban', 'do', 'nài', '<end_of_verso>']\n",
      "['che', 'la', 've', 'ra', 'ce', 'vi', 'a ab', 'ban', 'do', 'nai']\n",
      "['ma', '<word_sep>', 'pòi', '<word_sep>', \"ch'i'\", '<word_sep>', 'fùi', '<word_sep>', 'al', '<word_sep>', 'piè', '<word_sep>', \"d'un\", '<word_sep>', 'còl', 'le', '<word_sep>', 'giùn', 'to', '<end_of_verso>']\n",
      "['ma', 'poi', \"ch'i'\", 'fui al', 'piè', \"d'un\", 'col', 'le', 'giun', 'to']\n",
      "['là', '<word_sep>', 'dò', 've', '<word_sep>', 'ter', 'mi', 'nà', 'va', '<word_sep>', 'quèl', 'la', '<word_sep>', 'vàl', 'le', '<end_of_verso>']\n",
      "['là', 'do', 've', 'ter', 'mi', 'na', 'va', 'quel', 'la', 'val', 'le']\n",
      "['che', '<word_sep>', \"m'à\", 've', 'a', '<word_sep>', 'di', '<word_sep>', 'pa', 'ù', 'ra', '<word_sep>', 'il', '<word_sep>', 'cor', '<word_sep>', 'com', 'pùn', 'to', '<end_of_verso>']\n",
      "['che', \"m'a\", 've', 'a', 'di', 'pa', 'u', 'ra il', 'cor', 'com', 'pun', 'to']\n"
     ]
    }
   ],
   "source": [
    "# printing syllabification without special tokens \n",
    "for line in divine_comedy_list[:15]:\n",
    "    syllables = syllabify_verse_prettify(line, special_tokens, tone_tagger)\n",
    "    size = len(syllables)\n",
    "    print(syllables, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reversed data for corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<end_of_verso>',\n",
       " 'ta',\n",
       " 'vi',\n",
       " '<word_sep>',\n",
       " 'stra',\n",
       " 'no',\n",
       " '<word_sep>',\n",
       " 'di',\n",
       " '<word_sep>',\n",
       " 'min',\n",
       " 'cam',\n",
       " '<word_sep>',\n",
       " 'del',\n",
       " '<word_sep>',\n",
       " 'zo',\n",
       " 'mez',\n",
       " '<word_sep>',\n",
       " 'nel',\n",
       " '<end_of_verso>',\n",
       " 'ra',\n",
       " 'scu',\n",
       " 'va<word_sep>o',\n",
       " 'sel',\n",
       " '<word_sep>',\n",
       " 'na',\n",
       " 'u',\n",
       " '<word_sep>',\n",
       " 'per',\n",
       " '<word_sep>',\n",
       " 'vai',\n",
       " 'tro',\n",
       " 'ri',\n",
       " '<word_sep>',\n",
       " 'mi',\n",
       " '<end_of_verso>',\n",
       " 'ta',\n",
       " 'ri',\n",
       " 'smar',\n",
       " '<word_sep>',\n",
       " 'ra',\n",
       " 'via<word_sep>e',\n",
       " '<word_sep>',\n",
       " 'ta',\n",
       " 'rit',\n",
       " 'di',\n",
       " '<word_sep>',\n",
       " 'la',\n",
       " '<word_sep>',\n",
       " 'ché',\n",
       " '<end_of_verso>',\n",
       " 'ra',\n",
       " 'du',\n",
       " '<word_sep>',\n",
       " 'sa',\n",
       " 'co',\n",
       " '<word_sep>',\n",
       " 'ra<word_sep>è',\n",
       " 'e',\n",
       " '<word_sep>',\n",
       " 'qual',\n",
       " '<word_sep>',\n",
       " 'dir',\n",
       " '<word_sep>',\n",
       " 'to<word_sep>a',\n",
       " 'quan',\n",
       " '<word_sep>',\n",
       " 'hi',\n",
       " 'a',\n",
       " '<end_of_verso>',\n",
       " 'te',\n",
       " 'for',\n",
       " '<word_sep>',\n",
       " 'spra<word_sep>e',\n",
       " 'gia<word_sep>e<word_sep>a',\n",
       " 'vag',\n",
       " 'sel',\n",
       " '<word_sep>',\n",
       " 'va',\n",
       " 'sel',\n",
       " '<word_sep>',\n",
       " 'sta',\n",
       " 'e',\n",
       " '<end_of_verso>',\n",
       " 'ra',\n",
       " 'pau',\n",
       " '<word_sep>',\n",
       " 'la',\n",
       " '<word_sep>',\n",
       " 'va',\n",
       " 'no',\n",
       " 'ri',\n",
       " '<word_sep>',\n",
       " 'sier',\n",
       " 'pen',\n",
       " '<word_sep>',\n",
       " 'nel',\n",
       " '<word_sep>',\n",
       " 'che',\n",
       " '<end_of_verso>',\n",
       " 'te']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dante_by_rev_syl.data_preparation import text_in_rev_syls\n",
    "divine_comedy_verse = text_in_rev_syls(divine_comedy)\n",
    "divine_comedy_verse[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reversed data for rhyme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start_of_canto>',\n",
       " '<start_of_terzina>',\n",
       " 'vi',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " 'scu',\n",
       " 'ra',\n",
       " '<end_of_verso>',\n",
       " 'ri',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " 'du',\n",
       " 'ra',\n",
       " '<end_of_verso>',\n",
       " 'for',\n",
       " 'te',\n",
       " '<end_of_verso>',\n",
       " 'pau',\n",
       " 'ra',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " 'mor',\n",
       " 'te',\n",
       " '<end_of_verso>',\n",
       " 'tro',\n",
       " 'vai',\n",
       " '<end_of_verso>',\n",
       " 'scor',\n",
       " 'te',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " \"v'in\",\n",
       " 'trai',\n",
       " '<end_of_verso>',\n",
       " 'pun',\n",
       " 'to',\n",
       " '<end_of_verso>',\n",
       " 'do',\n",
       " 'nai',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " 'giun',\n",
       " 'to',\n",
       " '<end_of_verso>',\n",
       " 'val',\n",
       " 'le',\n",
       " '<end_of_verso>',\n",
       " 'pun',\n",
       " 'to',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " 'spal',\n",
       " 'le',\n",
       " '<end_of_verso>',\n",
       " 'ne',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " 'cal',\n",
       " 'le',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " 'que',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " 'ra',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " 'pie',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " 'na',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " 'ri',\n",
       " 'va',\n",
       " '<end_of_verso>',\n",
       " 'gua',\n",
       " 'ta',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>',\n",
       " '<start_of_terzina>',\n",
       " 'gi',\n",
       " 'va',\n",
       " '<end_of_verso>',\n",
       " 'pas',\n",
       " 'so',\n",
       " '<end_of_verso>',\n",
       " 'vi',\n",
       " 'va',\n",
       " '<end_of_verso>',\n",
       " '<end_of_terzina>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dante_by_rev_syl.data_preparation import text_in_syls_rhyme\n",
    "divine_comedy_rhyme = text_in_syls_rhyme(divine_comedy)\n",
    "divine_comedy_rhyme[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model(name, vocab_size, seq_length, embedding_dim=64, rnn_type='lstm', rnn_units=512, learning_rate=0.01):\n",
    "\n",
    "    model = tf.keras.Sequential(name=name)\n",
    "\n",
    "    model.add(tf.keras.layers.Input((seq_length,), name='input'))\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, name='embedding'))\n",
    "    if rnn_type == 'lstm':\n",
    "        model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          dropout=0.3,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          name='last_lstm')\n",
    "        )\n",
    "    elif rnn_type == 'gru':\n",
    "        model.add(tf.keras.layers.GRU(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          dropout=0.3,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          name='last_gru')\n",
    "        )\n",
    "    elif rnn_type == '2lstm':\n",
    "        model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          dropout=0.3,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          name='first_lstm')\n",
    "        )\n",
    "        model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          dropout=0.3,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          name='last_lstm')\n",
    "        )\n",
    "\n",
    "    elif rnn_type == '2gru':\n",
    "        model.add(tf.keras.layers.GRU(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          dropout=0.3,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          name='first_gru')\n",
    "        )\n",
    "        model.add(tf.keras.layers.GRU(rnn_units,\n",
    "                          return_sequences=True,\n",
    "                          dropout=0.3,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          name='last_gru')\n",
    "        )\n",
    "#    model.add(tf.keras.layers.Dense(128, activation='relu', name='dense'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(vocab_size, activation='softmax', name='output'))\n",
    "\n",
    "#    model.add(tf.keras.layers.Dense(vocab_size, name='output'))\n",
    "    \n",
    "    \n",
    "#    def loss(labels, logits):\n",
    "#        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "#    model.compile(loss=loss, metrics=\"accuracy\", optimizer=optimizer)\n",
    "    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\", optimizer=optimizer)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "#build_model(....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained the model, in this case we are using a pretrained model, we can generate our text.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dante_by_tonedrev_syl.data_preparation import text_in_rev_syls\n",
    "from dante_by_tonedrev_syl.text_processing import prettify_text, special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
