{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divina Commedia Dataset Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to showcase our project results here a demo to run our main script \"generete_dante.py\" and see our Canto printed out with all the requirements needed as:\n",
    "- terzine structure \n",
    "- hendecasyllable verse\n",
    "- rhyme scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PROCESSING\n",
    "We first want to show how we performed some data processing in order to get our latest canto, for this reason in the following session we will share different types of Divina Commedia text in order to all the phases we processed before to get out CANTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/allegraadinolfi/Desktop/UNIBO/DEEP LEARNING/deep_comedy/dante_by_tonedrev_syl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dante_by_tonedrev_syl.text_processing import clean_comedy, special_tokens, prettify_text\n",
    "from dante_by_tonedrev_syl.tone import ToneTagger\n",
    "from dante_by_tonedrev_syl.syllabification import syllabify_verse, syllabify_verse_prettify\n",
    "from dante_by_rev_syl.data_preparation import text_in_rev_syls\n",
    "from dante_by_rev_syl.data_preparation import text_in_syls_rhyme\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "working_dir = os.path.join(os.path.abspath('.'), 'dante_by_tonedrev_syl')\n",
    "divine_comedy_file = os.path.join(os.path.dirname(working_dir), \"divina_commedia\", \"divina_commedia_accent_UTF-8.txt\")\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_canto>\n",
      "<start_of_terzina>\n",
      "nel <word_sep> mezzo <word_sep> del <word_sep> cammin <word_sep> di <word_sep> nostra <word_sep> vita <end_of_verso>\n",
      "mi <word_sep> ritrovai <word_sep> per <word_sep> una <word_sep> selva <word_sep> oscura <end_of_verso>\n",
      "ché <word_sep> la <word_sep> diritta <word_sep> via <word_sep> era <word_sep> smarrita <end_of_verso>\n",
      "<end_of_terzina>\n",
      "<start_of_terzina>\n",
      "ahi <word_sep> quanto <word_sep> a <word_sep> dir <word_sep> qual <word_sep> era <word_sep> è <word_sep> cosa <word_sep> dura <end_of_verso>\n",
      "esta <word_sep> selva <word_sep> selvaggia <word_sep> e <word_sep\n",
      "Special tokens used:  dict_values(['<start_of_canto>', '<end_of_canto>', '<start_of_terzina>', '<end_of_terzina>', '<end_of_verso>', '<word_sep>'])\n"
     ]
    }
   ],
   "source": [
    "#reading the original divine comedy \n",
    "with open(divine_comedy_file,\"r\") as f:\n",
    "    divine_comedy = f.read()\n",
    "#cleaning the divine comedy with our functions    \n",
    "divine_comedy = clean_comedy(divine_comedy, special_tokens)\n",
    "print(divine_comedy[:600])\n",
    "print(\"Special tokens used: \", special_tokens.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ToneNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 32)            1088      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 512)               591872    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 608,350\n",
      "Trainable params: 608,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Tone Tagger\n",
    "tone_tagger = ToneTagger()\n",
    "tone_tagger.model_tone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(tone_tagger.model_tone, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.plugins import projector\n",
    "%load_ext tensorboard\n",
    "# Set up a logs directory, so Tensorboard knows where to look for files\n",
    "log_dir=os.path.join('/tmp', 'emb_visual_tensorboard')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Save Labels separately on a line-by-line manner.\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "    for char in tone_tagger.char2idx.keys():\n",
    "        f.write('\"{}\"\\n'.format(char))\n",
    "\n",
    "# Save the weights we want to analyse as a variable. Note that the first\n",
    "# value represents any unknown word, which is not in the metadata, so\n",
    "# we will remove that value.\n",
    "weights = tf.Variable(tone_tagger.model_tone.get_layer('embedding').get_weights()[0])\n",
    "# Create a checkpoint from embedding, the filename and key are\n",
    "# name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# Set up config\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 20727), started 0:09:40 ago. (Use '!kill 20727' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-54a94e19efb32a72\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-54a94e19efb32a72\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir /tmp/emb_visual_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cànto \n",
      "\n",
      "nel mèzzo del cammìn di nòstra vìta \n",
      "\n",
      "mi ritrovài per ùna sèlva oscùra \n",
      "\n",
      "ché la dirìtta vìa èra smàrrita \n",
      "\n",
      "\n",
      "\n",
      "àhi quànto a dir quàl èra è còsa dùra \n",
      "\n",
      "èsta sèlva selvàggia e àspra e fòrte \n",
      "\n",
      "che nel pènsier rinòva la paùra \n",
      "\n"
     ]
    }
   ],
   "source": [
    "divine_comedy_prettified = prettify_text(divine_comedy, special_tokens)\n",
    "#divine_comedy = remove_all_punctuation(divine_comedy)\n",
    "\n",
    "divine_comedy_verses = divine_comedy_prettified.split('\\n')[:8]\n",
    "\n",
    "for verse in divine_comedy_verses:\n",
    "    for w in verse.split():\n",
    "        print(tone_tagger.tone(w), flush=True, end=' ')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syllabified Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nel', '<word_sep>', 'mèz', 'zo', '<word_sep>', 'del', '<word_sep>', 'cam', 'mìn', '<word_sep>', 'di', '<word_sep>', 'nò', 'stra', '<word_sep>', 'vì', 'ta', '<end_of_verso>']\n",
      "['mi', '<word_sep>', 'ri', 'tro', 'vài', '<word_sep>', 'per', '<word_sep>', 'ù', 'na', '<word_sep>', 'sèl', 'va<word_sep>o', 'scù', 'ra', '<end_of_verso>']\n",
      "['ché', '<word_sep>', 'la', '<word_sep>', 'di', 'rìt', 'ta', '<word_sep>', 'vì', 'a', '<word_sep>', 'è', 'ra', '<word_sep>', 'smàr', 'ri', 'ta', '<end_of_verso>']\n",
      "['àhi', '<word_sep>', 'quàn', 'to', '<word_sep>', 'a', '<word_sep>', 'dir', '<word_sep>', 'quàl', '<word_sep>', 'è', 'ra', '<word_sep>', 'è', '<word_sep>', 'cò', 'sa', '<word_sep>', 'dù', 'ra', '<end_of_verso>']\n",
      "['è', 'sta', '<word_sep>', 'sèl', 'va', '<word_sep>', 'sel', 'vàg', 'gia<word_sep>e<word_sep>à', 'spra', '<word_sep>', 'e', '<word_sep>', 'fòr', 'te', '<end_of_verso>']\n",
      "['che', '<word_sep>', 'nel', '<word_sep>', 'pèn', 'sier', '<word_sep>', 'ri', 'nò', 'va', '<word_sep>', 'la', '<word_sep>', 'pa', 'ù', 'ra', '<end_of_verso>']\n",
      "['tan', \"t'\", '<word_sep>', 'è', '<word_sep>', 'a', 'mà', 'ra', '<word_sep>', 'che', '<word_sep>', 'pò', 'co', '<word_sep>', 'è', '<word_sep>', 'più', '<word_sep>', 'mòr', 'te', '<end_of_verso>']\n",
      "['ma', '<word_sep>', 'per', '<word_sep>', 'trat', 'tàr', '<word_sep>', 'del', '<word_sep>', 'ben', '<word_sep>', \"ch'i'\", '<word_sep>', 'vi', '<word_sep>', 'tro', 'vài', '<end_of_verso>']\n",
      "['di', 'rò', '<word_sep>', 'de', '<word_sep>', \"l'àl\", 'tre', '<word_sep>', 'cò', 'se', '<word_sep>', \"ch'i'\", '<word_sep>', \"v'ho\", '<word_sep>', 'scòr', 'te', '<end_of_verso>']\n",
      "['io', '<word_sep>', 'non', '<word_sep>', 'so', '<word_sep>', 'ben', '<word_sep>', 'ri', 'dìr', '<word_sep>', \"com'\", '<word_sep>', \"i'\", '<word_sep>', \"v'in\", 'trài', '<end_of_verso>']\n",
      "['tan', \"t'\", '<word_sep>', 'è', 'ra', '<word_sep>', 'pièn', '<word_sep>', 'di', '<word_sep>', 'sòn', 'no', '<word_sep>', 'a', '<word_sep>', 'quèl', '<word_sep>', 'pùn', 'to', '<end_of_verso>']\n",
      "['che', '<word_sep>', 'la', '<word_sep>', 've', 'rà', 'ce', '<word_sep>', 'vì', 'a<word_sep>ab', 'ban', 'do', 'nài', '<end_of_verso>']\n",
      "['ma', '<word_sep>', 'pòi', '<word_sep>', \"ch'i'\", '<word_sep>', 'fùi', '<word_sep>', 'al', '<word_sep>', 'piè', '<word_sep>', \"d'un\", '<word_sep>', 'còl', 'le', '<word_sep>', 'giùn', 'to', '<end_of_verso>']\n",
      "['là', '<word_sep>', 'dò', 've', '<word_sep>', 'ter', 'mi', 'nà', 'va', '<word_sep>', 'quèl', 'la', '<word_sep>', 'vàl', 'le', '<end_of_verso>']\n",
      "['che', '<word_sep>', \"m'à\", 've', 'a', '<word_sep>', 'di', '<word_sep>', 'pa', 'ù', 'ra<word_sep>il', '<word_sep>', 'cor', '<word_sep>', 'com', 'pùn', 'to', '<end_of_verso>']\n"
     ]
    }
   ],
   "source": [
    "divine_comedy_list = divine_comedy.split(\"\\n\")\n",
    "divine_comedy_list = [ line for line in divine_comedy_list if line.strip() not in special_tokens.values() ]\n",
    "for line in divine_comedy_list[:15]:\n",
    "    syllables = syllabify_verse(line, special_tokens, tone_tagger)\n",
    "    size = len(syllables)\n",
    "    print(syllables, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nel', 'mèz', 'zo', 'del', 'cam', 'mìn', 'di', 'nò', 'stra', 'vì', 'ta']\n",
      "['mi', 'ri', 'tro', 'vài', 'per', 'ù', 'na', 'sèl', 'va o', 'scù', 'ra']\n",
      "['ché', 'la', 'di', 'rìt', 'ta', 'vì', 'a', 'è', 'ra', 'smàr', 'ri', 'ta']\n",
      "['àhi', 'quàn', 'to', 'a', 'dir', 'quàl', 'è', 'ra', 'è', 'cò', 'sa', 'dù', 'ra']\n",
      "['è', 'sta', 'sèl', 'va', 'sel', 'vàg', 'gia e à', 'spra', 'e', 'fòr', 'te']\n",
      "['che', 'nel', 'pèn', 'sier', 'ri', 'nò', 'va', 'la', 'pa', 'ù', 'ra']\n",
      "['tan', \"t'\", 'è', 'a', 'mà', 'ra', 'che', 'pò', 'co', 'è', 'più', 'mòr', 'te']\n",
      "['ma', 'per', 'trat', 'tàr', 'del', 'ben', \"ch'i'\", 'vi', 'tro', 'vài']\n",
      "['di', 'rò', 'de', \"l'àl\", 'tre', 'cò', 'se', \"ch'i'\", \"v'ho\", 'scòr', 'te']\n",
      "['io', 'non', 'so', 'ben', 'ri', 'dìr', \"com'\", \"i'\", \"v'in\", 'trài']\n",
      "['tan', \"t'\", 'è', 'ra', 'pièn', 'di', 'sòn', 'no', 'a', 'quèl', 'pùn', 'to']\n",
      "['che', 'la', 've', 'rà', 'ce', 'vì', 'a ab', 'ban', 'do', 'nài']\n",
      "['ma', 'pòi', \"ch'i'\", 'fùi', 'al', 'piè', \"d'un\", 'còl', 'le', 'giùn', 'to']\n",
      "['là', 'dò', 've', 'ter', 'mi', 'nà', 'va', 'quèl', 'la', 'vàl', 'le']\n",
      "['che', \"m'à\", 've', 'a', 'di', 'pa', 'ù', 'ra il', 'cor', 'com', 'pùn', 'to']\n"
     ]
    }
   ],
   "source": [
    "# printing syllabification without special tokens \n",
    "for line in divine_comedy_list[:15]:\n",
    "    syllables = syllabify_verse_prettify(line, special_tokens, tone_tagger)\n",
    "    size = len(syllables)\n",
    "    print(syllables, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reversed data for corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<end_of_verso> ta vi <word_sep> stra no <word_sep> di <word_sep> min cam <word_sep> del <word_sep> zo mez <word_sep> nel \n",
      "\n",
      "<end_of_verso> ra scu va<word_sep>o sel <word_sep> na u <word_sep> per <word_sep> vai tro ri <word_sep> mi \n",
      "\n",
      "<end_of_verso> ta ri smar <word_sep> ra via<word_sep>e <word_sep> ta rit di <word_sep> la <word_sep> ché \n",
      "\n",
      "<end_of_verso> "
     ]
    }
   ],
   "source": [
    "divine_comedy_syls = text_in_rev_syls(divine_comedy)\n",
    "for syl in divine_comedy_syls[:50]:\n",
    "    if syl == '<end_of_verso>':\n",
    "        print('\\n')\n",
    "    print(syl, flush=True, end=' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rhymes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_canto>\n",
      "<start_of_terzina>\n",
      "vi ta <end_of_verso>\n",
      "scu ra <end_of_verso>\n",
      "ri ta <end_of_verso>\n",
      "<end_of_terzina>\n",
      "<start_of_terzina>\n",
      "du ra <end_of_verso>\n",
      "for te <end_of_verso>\n",
      "pau ra <end_of_verso>\n",
      "<end_of_terzina>\n",
      "<start_of_terzina>\n",
      "mor te <end_of_verso>\n",
      "tro vai <end_of_verso>\n",
      "scor te <end_of_verso>\n",
      "<end_of_terzina>\n",
      "<start_of_terzina>\n",
      "v'in trai <end_of_verso>\n",
      "pun to <end_of_verso>\n",
      "do nai <end_of_verso>\n",
      "<end_of_terzina>\n",
      "<start_of_terzina>\n",
      "giun to <end_of_verso>\n",
      "val "
     ]
    }
   ],
   "source": [
    "divine_comedy_rhyme = text_in_syls_rhyme(divine_comedy)\n",
    "for syl in divine_comedy_rhyme[:50]:\n",
    "    if syl not in special_tokens.values():\n",
    "        print(syl, end=' ')\n",
    "    else:\n",
    "        print(syl)\n",
    "#divine_comedy_rhyme[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Hello, world!</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<h1>Hello, world!</h1>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Hello, <span style=\"background-color: #6495ED\">blue</span>!</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<h1>Hello, <span style=\"background-color: #6495ED\">blue</span>!</h1>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generetebytonedrevverse do the same for the rhyme and only the plot for the toned \n",
    "# Path where the vocab is saved\n",
    "from utils import save_vocab, load_vocab\n",
    "working_dir = os.path.join(os.path.abspath('.'), 'dante_by_rev_syl')\n",
    "import tensorflow as tf\n",
    "\n",
    "logs_dir = os.path.join(working_dir, 'logs')\n",
    "os.makedirs(logs_dir, exist_ok = True) \n",
    "vocab_file = os.path.join(logs_dir, 'vocab_verse.json')\n",
    "\n",
    "vocab, idx2syl, syl2idx = load_vocab(vocab_file)\n",
    "\n",
    "# Length of the vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "# Path where the model is saved\n",
    "\n",
    "models_dir = os.path.join(working_dir, 'models')\n",
    "os.makedirs(models_dir, exist_ok = True) \n",
    "model_file_verse = os.path.join(models_dir, \"dante_by_rev_syl_verse_model.h5\")\n",
    "\n",
    "model_verse = tf.keras.models.load_model(model_file_verse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VerseNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 256)          1481472   \n",
      "_________________________________________________________________\n",
      "last_lstm (LSTM)             (None, 100, 1024)         5246976   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 100, 5787)         5931675   \n",
      "=================================================================\n",
      "Total params: 12,660,123\n",
      "Trainable params: 12,660,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_verse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "#plot\n",
    "tf.keras.utils.plot_model(model_verse, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to generate the text\n",
    "divine_comedy_rhyme = text_in_syls_rhyme(divine_comedy)\n",
    "#index_eoc = divine_comedy_rhyme.index(special_tokens['END_OF_CANTO']) + 1\n",
    "indexes = [i for i, x in enumerate(divine_comedy_rhyme) if x == special_tokens['END_OF_CANTO']]\n",
    "index_eoc = np.random.choice(indexes) + 1\n",
    "start_seq_rhyme = divine_comedy_rhyme[index_eoc - SEQ_LENGTH_RHYME:index_eoc]\n",
    "\n",
    "\n",
    "divine_comedy_verse = text_in_rev_syls(divine_comedy)\n",
    "indexes = [i for i, x in enumerate(divine_comedy_verse) if x == special_tokens['END_OF_VERSO'] and i > SEQ_LENGTH_VERSE]\n",
    "index_eov = np.random.choice(indexes)\n",
    "start_seq_verse = divine_comedy_verse[index_eov - SEQ_LENGTH_VERSE:index_eov]\n",
    "\n",
    "\n",
    "\n",
    "generated_text = generate_text(model_rhyme, model_verse, special_tokens, vocab_size_rhyme, vocab_size_verse, syl2idx_rhyme, idx2syl_rhyme, syl2idx_verse, idx2syl_verse, SEQ_LENGTH_RHYME, SEQ_LENGTH_VERSE, start_seq_rhyme, start_seq_verse, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained the model, in this case we are using a pretrained model, we can generate our text.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
